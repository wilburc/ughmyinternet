{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Messenger Data Wrangling\n",
    "\n",
    "Python code that creates nice little .csv’s from a message history in your [Facebook Data Dump](https://www.facebook.com/settings).\n",
    "\n",
    "### Note: This was written in Jan 2018. As of right now (Aug 2018), this will no longer work. I’m working on an update to this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the actual conversation into a nicer file\n",
    "\n",
    "Most data dumps come in .html, to provide a nice web interface for viewing your data. Try opening one of your conversation .html files and you’ll find that you can read the entire thing in your browser, starting from the beginning. This is pretty cool but not THE BEST for DOING THE DATA.\n",
    "\n",
    "Some things about the data dump:\n",
    "* Gifs/emojis are not captured and are represented as empty messages (Images are too, kinda).\n",
    "* Data can be missing. I’ve had entire months excluded from dumps. Hello??? Mark?? why\n",
    "* __The format changes every now and then (I’ve been doing this periodically for the past few years). This code was adjusted to process a dump from Jan 2018, but I often have to make a few changes. It isn’t too hard though, you can do it!!!__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fname = '119.html' # fname is the file name of the .html file that contains the conversation (group chat OR individual)\n",
    "                   # that you want to analyze. These can be found in the “Messages” folder in the data dump\n",
    "f = open(fname)\n",
    "soup = BeautifulSoup(f,'lxml') # This can take a few minutes, depending on the size of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A message has 3 parts: the person who says it, the meta data (the time it was sent), and the message itself.\n",
    "# We’ll extract these 3 components into their own arrays (the length of each of these arrays represents the \n",
    "# number of messages in the conversation). This part might take a while as well sorry\n",
    "\n",
    "people = [p.text.encode('utf-8') for p in soup.find_all(\"span\",\"user\")]\n",
    "times = [t.text.encode('utf-8') for t in soup.find_all(\"span\",\"meta\")]\n",
    "messages = [m.text.encode('utf-8') for m in soup.find_all(\"p\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes the data dump bugs out and has twice the <p> tags that it should (idk lol)\n",
    "# This will take care of that \n",
    "\n",
    "if len(messages) == len(times) * 2:\n",
    "    messages = messages[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the actual “conversation” which we’ll represent as a list of “message objects” (dicts)\n",
    "\n",
    "conversation = []\n",
    "for i in range(len(people)): # theoretically you could replace “people” with “messages” or “times”\n",
    "    person = people[i]\n",
    "    time = times[i]\n",
    "    message = messages[i]\n",
    "    thing = {\n",
    "        'Person':person,\n",
    "        'Time':time,\n",
    "        'Message':message\n",
    "    }\n",
    "    conversation.append(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you know shit’s going down when you convert to dataframe\n",
    "\n",
    "df = pd.DataFrame(data=conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FB stores the time-data as one giant string, we’ll separate that into 'date' and 'time' columns,\n",
    "# “time” being the hour and the minute, and “date” being the year/month/date\n",
    "\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='%A, %B %d, %Y at %I:%M%p %Z') \n",
    "df['Date'] = df['Time'].apply(lambda x: x.strftime('%Y/%m/%d'))\n",
    "df['Time'] = df['Time'].apply(lambda x: x.strftime('%H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just in case. We’ll sort the messages chronologically. \n",
    "# Why not sort by time? Because you likely send multiple messages in the same minute, and that’s \n",
    "# the lowest level of granularity provided by FB.\n",
    "\n",
    "## `ascending=False` if you want the oldest message at the top, `True` for the opposite.\n",
    "\n",
    "df = df.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We’ll export this as “conversation.csv,” which is basically your entire conversation history but in a nicer\n",
    "# format.\n",
    "\n",
    "# Just hold onto this for your on reference or for creating other files\n",
    "\n",
    "\n",
    "df.to_csv('conversation.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make aggregated data files from this conversation\n",
    "\n",
    "While the message content is probably the most interesting part, I’m hesitant to really use it because your messages PROBABLY contain some more sensitive data which you don’t want uploaded to THE CLOUD where everyone can see it. We’ll use the `conversation.csv` that we created earlier to make some files for *meta* analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember this?\n",
    "df = pd.read_csv('conversation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll start by exploring who sent how many messages, at the daily level. This is kinda boring for individual chats but it’s *enlightening* for group chats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = df['Person'].unique()\n",
    "counts = defaultdict(lambda:{user:0 for user in users}) # obviously defaultdict isn’t necessary but it’s cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We’ll get the daterange of the conversation and “manually” get each user’s message counts for that day\n",
    "# Yes this can probably be achieved with a groupby lol\n",
    "\n",
    "start = df['Date'].min()\n",
    "end = df['Date'].max()\n",
    "for date in pd.date_range(start=start,end=end):\n",
    "    date_df = df[df['Date'] == date.strftime('%Y/%m/%d')]\n",
    "    for user in users:\n",
    "        counts[date][user] = len(date_df[date_df['Person'] == user])\n",
    "        counts[date]['Total'] = len(date_df)\n",
    "counts = pd.DataFrame.from_dict(counts)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just because\n",
    "\n",
    "counts = counts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts['Date'] = counts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and now we have a handy csv of every chat member’s daily message counts\n",
    "\n",
    "counts.to_csv('datecounts.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making even MORE files from this\n",
    "\n",
    "Because we have time/date columns, we can actually make even more granular message-count files from `conversation.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('conversation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll start by getting message counts at the day-of-week level, as well as the hour-of-day level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add new columns (day of week + hour of day) to the conversation df\n",
    "\n",
    "def date_to_day(date):\n",
    "    ''' gets the day of week from a date'''\n",
    "    ''' isn’t that cool                 '''\n",
    "    date = datetime.strptime(date,'%Y/%m/%d')\n",
    "    day = date.weekday()\n",
    "    return day\n",
    "\n",
    "df['Day'] = df.apply(lambda row: date_to_day(row['Date']), axis=1)\n",
    "df['Hour'] = df.apply(lambda row: row['Time'][0:2], axis=1) # it feels a little dirty doing it this way but w/e lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## we can create two new dataframes from this \n",
    "\n",
    "hour_day = df.groupby([df['Day'],df['Hour']]).count()\n",
    "hour_date = df.groupby([df['Date'],df['Hour']]).count()\n",
    "\n",
    "hour_day = hour_day.reset_index()\n",
    "hour_date = hour_date.reset_index()\n",
    "\n",
    "# i know the column is called “time” but it really represents the total number of messages lol\n",
    "hour_day = hour_day[['Day','Hour','Time']]\n",
    "hour_date = hour_date[['Hour','Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## honestly i forgot what this does/why i did it this way but just…\n",
    "## just… do it. it’s actually pretty straightforward. I just remember\n",
    "## this took forever to type and could probably be wrapped in a function\n",
    "\n",
    "hour1 = hour_day[['Hour','Time']]\n",
    "hour2 = hour_date[['Hour','Time']]\n",
    "\n",
    "hour_total = hour1.groupby([hour1['Hour']]).sum()\n",
    "hour_average = hour2.groupby([hour2['Hour']]).mean()\n",
    "hour_median = hour2.groupby([hour2['Hour']]).median()\n",
    "\n",
    "hour_total = hour_total.reset_index()\n",
    "hour_average = hour_average.reset_index()\n",
    "hour_median = hour_median.reset_index()\n",
    "\n",
    "hour_total = hour_total.rename(columns={'Time':'Total'})\n",
    "hour_average = hour_average.rename(columns={'Time':'Average'})\n",
    "hour_median = hour_median.rename(columns={'Time':'Median'})\n",
    "\n",
    "hour_stats = hour_total.merge(hour_average)\n",
    "hour_stats = hour_stats.merge(hour_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = df.groupby([df['Day'],df['Date']]).count()\n",
    "counts = counts.reset_index()\n",
    "days = counts[['Day','Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_average = days.groupby([days['Day']]).mean()\n",
    "day_total = days.groupby([days['Day']]).sum()\n",
    "day_median = days.groupby([days['Day']]).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this is more of the same—basically just cleaning up these dataframes. \n",
    "## the rest of this notebook is just going through the same steps but for different metrics\n",
    "\n",
    "day_average = day_average.reset_index()\n",
    "day_total = day_total.reset_index()\n",
    "day_median = day_median.reset_index()\n",
    "\n",
    "day_average = day_average.rename(columns={'Time':'Average'})\n",
    "day_total = day_total.rename(columns={'Time':'Total'})\n",
    "day_median = day_median.rename(columns={'Time':'Median'})\n",
    "\n",
    "day_stats = day_total.merge(day_average)\n",
    "day_stats = day_stats.merge(day_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def daynum_to_daystr(daynum):\n",
    "    days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "    try:\n",
    "        day = days[int(daynum)]\n",
    "    except ValueError:\n",
    "        day = daynum\n",
    "    return day\n",
    "                \n",
    "day_stats['Day'] = day_stats.apply(lambda row: daynum_to_daystr(row['Day']), axis=1)\n",
    "day_stats.to_csv('day_stats.csv',index=False)\n",
    "hour_stats.to_csv('hour_stats.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_day_date = df.groupby([df['Date'],df['Hour'],df['Day']]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_day_date = hour_day_date.reset_index()\n",
    "hour_day_date = hour_day_date[['Hour','Day','Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdd_total = hour_day_date.groupby([hour_day_date['Hour'],hour_day_date['Day']]).sum()\n",
    "hdd_average = hour_day_date.groupby([hour_day_date['Hour'],hour_day_date['Day']]).mean()\n",
    "hdd_median = hour_day_date.groupby([hour_day_date['Hour'],hour_day_date['Day']]).median()\n",
    "\n",
    "hdd_total = hdd_total.reset_index()\n",
    "hdd_average = hdd_average.reset_index()\n",
    "hdd_median = hdd_median.reset_index()\n",
    "\n",
    "hdd_total = hdd_total.rename(columns={'Time':'Total'})\n",
    "hdd_average = hdd_average.rename(columns={'Time':'Average'})\n",
    "hdd_median = hdd_median.rename(columns={'Time':'Median'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdd = hdd_total.merge(hdd_average)\n",
    "hdd = hdd.merge(hdd_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdd = hdd.sort_values(by=['Day','Hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def daynum_to_daystr(daynum):\n",
    "    days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "    try:\n",
    "        day = days[int(daynum)]\n",
    "    except ValueError:\n",
    "        day = daynum\n",
    "    return day\n",
    "\n",
    "def hournum_to_hourstr(hournum):\n",
    "    t1 = datetime.strptime(hournum,'%H')\n",
    "    t2 = t1.strftime('%I %p')\n",
    "    return t2\n",
    "\n",
    "hdd['Day'] = hdd.apply(lambda row: daynum_to_daystr(row['Day']),axis=1)\n",
    "hdd['DayHour'] = hdd.apply(lambda row: row['Day'] + ' ' + hournum_to_hourstr(row['Hour']), axis=1)\n",
    "hdd.to_csv('hdd.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
